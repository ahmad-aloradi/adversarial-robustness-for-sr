# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  # This MUST be first. It tells Hydra to start building the config from this file.
  - _self_  
  - override /callbacks: default.yaml
  - override /datamodule: datasets/voxceleb.yaml
  - override /module: sv.yaml
  - override /trainer: gpu.yaml
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["voxceleb", "pruning",  "sv", "ecapa", "cyclicLR"]
seed: 42

trainer:
  min_epochs: 10
  max_epochs: 15
  gradient_clip_val: 1.0
  num_sanity_val_steps: 0

callbacks:
  model_checkpoint:
    mode: max
    monitor: ${replace:"valid/__metric__"}
  early_stopping:
    mode: max
    monitor: ${replace:"valid/__metric__"}
    patience: 8
    
  model_pruning:
    _target_: src.callbacks.pruning.bregman.bregman_pruner.BregmanPruner
    sparsity_threshold: 1e-30
    collect_metrics: true
    verbose: 2
    # Global lambda scheduler configuration.
    lambda_scheduler:
      _target_: src.callbacks.pruning.bregman.lambda_scheduler.LambdaScheduler
      initial_lambda: 1e-6
      target_sparsity: 0.9
      adjustment_factor: 1.05
      min_lambda: 1e-9
      max_lambda: 1e-2

datamodule:
  dataset:
    max_duration: 2.0

  loaders:
    train:
      batch_size: 16
    valid:
      batch_size: 16
    test:
      batch_size: 8
    enrollment:
      batch_size: 1

### score normalization ###
module:
  normalize_test_scores: False

  lr_scheduler: 
    scheduler:
        _target_: torch.optim.lr_scheduler.CyclicLR
        base_lr: 1e-6
        max_lr: ${module.optimizer.lr}
        step_size_up: ${oc.eval:"int(2 * 1176000 / ${datamodule.loaders.train.batch_size})"}
    extras:
      monitor: ${replace:"valid/__metric__"}
      interval: step
      frequency: 1

  optimizer:
  # Option1 Linbreg  
    _target_: src.callbacks.pruning.bregman.bregman_optimizers.LinBreg
    _partial_: True
    momentum: 0.0
    lr: 1e-4

  # Option2 Adabreg
    # _target_: src.callbacks.pruning.bregman.bregman_optimizers.AdaBreg
    # _partial_: true
    # lr: 1e-4

  model:
    pruning_groups:
      # Group 1: Prunable Convolutional Layers
      # Adheres to the code's requirements by specifying both `layer_types`
      # and a valid regular expression pattern.
      - name: prunable_weights
        # MANDATORY: The code requires this key to be present.
        layer_types:
          - 'torch.nn.Conv1d'
          - 'torch.nn.Conv2d'
        # REGEX: This pattern matches module names ending in '.conv.conv'.
        # '.*' matches any character.
        # '\.' matches a literal dot.
        # '$' asserts this is the end of the string.
        module_name_patterns:
          - '.*\.conv\.conv$'
          - '.*\.conv1\.conv$'
          - '.*\.conv2\.conv$'
          - 'fc.conv'
        optimizer_settings:
          reg:
            _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegL1L2Conv
            lamda: 0.001 # This is the initial lambda
          lambda_scale: 1.0 # Scale factor for the global lambda

        pruning_config:
          sparsity_rate: 0.90

      # Group 2: BatchNorm Layers (Not Pruned)
      # Explicitly matches BatchNorm layers to assign them no regularization.
      - name: batchnorm_params
        layer_types:
          - 'torch.nn.BatchNorm1d'
        module_name_patterns:
          - '.*\.norm\.norm$'
          - 'asp_bn.norm'
        optimizer_settings:
          reg:
            _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegNone
          lambda_scale: 0.0 # Scale factor for the global lambda

      # Group 3: Unpruned Fallback (MUST BE LAST)
      # This group does not need layer_types because the logic to select it
      # should be separate (i.e., it catches what's left over).
      - name: unpruned_fallback
        # Mark this as the fallback so your logic can handle it.
        is_fallback: True
        module_name_patterns:
          - ".*" # Regex for "match anything"
        optimizer_settings:
          reg:
            _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegL1
            lamda: 0.001
          lambda_scale: .2 # Scale factor for the global lambda

        pruning_config:
          sparsity_rate: 0.90


# logger:
#   wandb:
#     tags: ${tags}
#     group: "mnist"
