# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /callbacks: default.yaml
  - override /datamodule: datasets/voxceleb.yaml
  - override /module: sv.yaml
  - override /trainer: gpu.yaml
  - override /logger: wandb.yaml


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["voxceleb", "pruning",  "sv", "ecapa", "cyclicLR"]
seed: 42

trainer:
  min_epochs: 10
  max_epochs: 15
  gradient_clip_val: 1.0
  num_sanity_val_steps: 0

callbacks:
  model_checkpoint:
    mode: max
    monitor: ${replace:"valid/__metric__"}
  early_stopping:
    mode: max
    monitor: ${replace:"valid/__metric__"}
    patience: 8
    
  model_pruning:
    _target_: src.callbacks.pruning.bregman.bregman_pruner.BregmanPruner
    sparsity_threshold: 1e-30
    collect_metrics: true
    verbose: 2
    # Lambda scheduler configuration remains here, as it's part of the callback's orchestration role.
    lambda_scheduler:
      _target_: src.callbacks.pruning.bregman.lambda_scheduler.LambdaScheduler
      _partial_: true  # Creates partial function - completed in BregmanPruner with optimizer
      warmup: 2
      increment: 0.05
      cooldown: 2
      target_sparsity: 0.8
      reg_param: "lamda"

datamodule:
  dataset:
    max_duration: 2.0

  loaders:
    train:
      batch_size: 16
    valid:
      batch_size: 16
    test:
      batch_size: 8
    enrollment:
      batch_size: 1

### score normalization ###
module:
  normalize_test_scores: False

  lr_scheduler: 
    scheduler:
        _target_: torch.optim.lr_scheduler.CyclicLR
        base_lr: 1e-6
        max_lr: ${module.optimizer.lr}
        step_size_up: ${oc.eval:"int(2 * 1176000 / ${datamodule.loaders.train.batch_size})"}
    extras:
      monitor: ${replace:"valid/__metric__"}
      interval: step
      frequency: 1

  optimizer:
  # Option1 Linbreg  
  #   _target_: src.callbacks.pruning.bregman.bregman_optimizers.LinBreg
  #   _partial_: True
  #   reg:
  #     _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegL1
  #     lamda: 1e-6  # Adjusted to a more reasonable value
  #     delta: 1.0  # This should match the callback's delta
  #   lr: 1e-4
  #   momentum: 0.0

  # Option2 Adabreg
    _target_: src.callbacks.pruning.bregman.bregman_optimizers.AdaBreg
    _partial_: true

  model:
    pruning_groups:
      - name: "convolutional_weights"
        # ECAPA-TDNN models use Conv1d layers.
        layer_types: ["torch.nn.Conv1d"]
        param_names: ["weight"]
        pruning_config:
          pruning_type: "structured"
          sparsity_rate: 0.80 # Initial sparsity: Prune 80% of conv channels
          structured_method: "channel"
        optimizer_settings:
          lr: 1e-4
          delta: 1.0
          eps: 1e-8
          reg:
            _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegL1L2Conv
            lamda: 1e-8

      - name: "linear_weights"
        layer_types: ["torch.nn.Linear"]
        param_names: ["weight"]
        pruning_config:
          pruning_type: "unstructured"
          sparsity_rate: 0.95 # Initial sparsity: Prune 95% of linear weights
        optimizer_settings:
          lr: 1e-4
          delta: 1.0
          eps: 1e-8
          reg:
            _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegL1
            lamda: 1e-8

      # This is a crucial fallback group. It catches all parameters not matched above
      # (e.g., all biases, batchnorm params, etc.) and applies no sparsity or regularization.
      - name: "fallback_group"
        is_fallback: True
        pruning_config:
          sparsity_rate: 0.0 # No initial sparsity
        optimizer_settings:
          lr: 1e-4
          delta: 1.0
          eps: 1e-8
          reg:
            _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegNone # No regularization

# logger:
#   wandb:
#     tags: ${tags}
#     group: "mnist"
