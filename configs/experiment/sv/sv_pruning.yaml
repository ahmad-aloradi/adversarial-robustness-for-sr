# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /callbacks: default.yaml
  - override /datamodule: datasets/voxceleb.yaml
  - override /module: sv.yaml
  - override /trainer: gpu.yaml
  - override /logger: wandb.yaml


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["voxceleb", "pruning",  "sv", "ecapa", "cyclicLR"]
seed: 42

trainer:
  min_epochs: 10
  max_epochs: 15
  gradient_clip_val: 1.0
  num_sanity_val_steps: 0

callbacks:
  model_checkpoint:
    mode: max
    monitor: ${replace:"valid/__metric__"}
  early_stopping:
    mode: max
    monitor: ${replace:"valid/__metric__"}
    patience: 8
    
  model_pruning:
    _target_: src.callbacks.pruning.bregman.bregman_pruner.BregmanPruner
    # Scheduler configuration
    scheduler_warmup: 0
    scheduler_cooldown: 0
    scheduler_increment: 0
    target_sparsity: 0.6
    # Sparse initialization
    sparse_init: true
    init_sparsity: 0.8
    # Parameter selection options:
    # "weights_only" - all parameters of modules with learnable params
    # "weights_no_bias" - only weight parameters, excluding biases (basic layers)
    # "comprehensive" - smart selection for modern architectures (LSTM, Transformers)
    # null - prune all model parameters
    parameter_selection: weights_only  # Best for modern architectures
    # Monitoring and verbosity
    collect_metrics: true
    verbose: 2

datamodule:
  dataset:
    max_duration: 2.0

  loaders:
    train:
      batch_size: 16
    valid:
      batch_size: 16
    test:
      batch_size: 8
    enrollment:
      batch_size: 1

### score normalization ###
module:
  normalize_test_scores: True
  scores_norm:
    embeds_metric_params:
      num_speakers_in_cohort: 6
      min_utts_per_speaker: 7
    scores_norm_params:
      topk: 1
      min_cohort_size: 2

  lr_scheduler: 
    scheduler:
        _target_: torch.optim.lr_scheduler.CyclicLR
        base_lr: 1e-6
        max_lr:  1e-4 #${module.optimizer.lr}
        step_size_up: ${oc.eval:"int(2 * 1176000 / ${datamodule.loaders.train.batch_size})"}
    extras:
      monitor: ${replace:"valid/__metric__"}
      interval: step
      frequency: 1

  optimizer:
    # _target_: src.callbacks.pruning.bregman.bregman_optimizers.LinBreg
    _target_: src.callbacks.pruning.bregman.bregman_optimizers.AdaBreg
    _partial_: True
    reg:
      _target_: src.callbacks.pruning.bregman.bregman_regularizers.RegL1
      lamda: 1e-8  # This should match the callback's lamda
      delta: 1.0  # This should match the callback's delta
    lr: 1e-4
    # momentum: 0.0

  # optimizer:
  #   lr: 1.0e-4
  #   weight_decay: 1.0e-5


# logger:
#   wandb:
#     tags: ${tags}
#     group: "mnist"
