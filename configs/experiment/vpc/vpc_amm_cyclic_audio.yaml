# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /callbacks: default.yaml
  - override /datamodule: datasets/vpc.yaml
  - override /module: vpc.yaml
  - override /trainer: gpu.yaml
  - override /logger: tensorboard.yaml
  - override /module/model: robust_audio.yaml


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["vpc25", "de-anonmyzation", "amm", "comprehensive"]
seed: 42

trainer:
  min_epochs: 10
  max_epochs: 25
  gradient_clip_val: 1.0
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1

callbacks:
  model_checkpoint:
    mode: min
    monitor: ${replace:"valid/__metric__"}/${module.metrics.valid_best.target_key}
  early_stopping:
    mode: min
    monitor: ${replace:"valid/__metric__"}/${module.metrics.valid_best.target_key}
    patience: 10

datamodule:
  dataset:
    max_duration: 10.0

  models: null

  loaders:
    train:
      batch_size: 16
    valid:
      batch_size: 16
    test:
      batch_size: 4
    enrollment:
      batch_size: 1

module:
  _target_: src.modules.audio_vpc.AudioVPCModel

  normalize_test_scores: True
      
  criterion:
    train_criterion:
      _target_: speechbrain.nnet.losses.LogSoftmaxWrapper
      loss_fn:
        _target_: speechbrain.nnet.losses.AdditiveAngularMargin
        margin: 0.15
        scale: 30.0

  lr_scheduler: 
    scheduler:
        _target_: torch.optim.lr_scheduler.CyclicLR
        base_lr: 1e-6
        max_lr: ${module.optimizer.lr}
        step_size_up: ${oc.eval:"int(2 * 104015 / ${datamodule.loaders.train.batch_size})"} # Full cycle every 4 epochs
    extras:
      monitor: ${replace:"valid/__metric__"}
      interval: step
      frequency: 1


  optimizer:
    lr: 1.0e-4
    weight_decay: 1.0e-5


# logger:
#   wandb:
#     tags: ${tags}
#     group: "mnist"
