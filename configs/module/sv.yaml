_target_: src.modules.sv.SpeakerVerification

defaults:
  - _self_

model:
  audio_processor:
    _target_: torch.nn.Identity

  audio_encoder:
    _target_: speechbrain.inference.EncoderClassifier.from_hparams
    source: "speechbrain/spkrec-ecapa-voxceleb"
    savedir: "local/.pretrained_models/spkrec-ecapa-voxceleb"
    run_opts:
      device: "cuda"

  # audio_processor:
  #   _target_: speechbrain.lobes.features.MFCC
  #   sample_rate: 16000
  #   win_length: 25                # in seconds
  #   hop_length: 10                # in seconds
  #   n_mels: 80                    # Number of mel filters
  #   n_mfcc: 80                    # 80-dimensional MFCCs
  #   deltas: False                 # No delta features
  #   left_frames: 0                # No context frames
  #   right_frames: 0               # No context frames
  #   f_min: 0                      # Minimum frequency
  #   f_max: 8000                   # Nyquist frequency for 16kHz

  # audio_encoder:
  #   _target_: speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
  #   input_size: 80

  audio_processor_kwargs:
    return_tensors: "pt"
    sampling_rate: 16000

### classifiers ###
  classifiers:
    # Common configuration
    config:
      num_classes: ${datamodule.num_classes}
      bottleneck_size: 512
      audio_embedding_size: 192  # ECAPA embedding size

    classifier:
      _target_: torch.nn.Sequential
      _args_:
        - _target_: torch.nn.Linear
          in_features: ${module.model.classifiers.config.audio_embedding_size}
          out_features: ${module.model.classifiers.config.num_classes}

### caching ###
  embedding_cache:
    max_size: 500000
    bypass_warmup: True
    
  # Enable score normalization
  normalize_test_scores: False

### criteria and metrics ###
criterion:
  available_criteria:    
    aam:
      _target_: speechbrain.nnet.losses.LogSoftmaxWrapper
      loss_fn:
        _target_: speechbrain.nnet.losses.AdditiveAngularMargin
        margin: 0.2
        scale: 30.0    
    cross_entropy:
      _target_: torch.nn.CrossEntropyLoss

  # Selected criterion (defaults to comprehensive)
  selected_criterion: aam
  train_criterion: ${module.criterion.available_criteria.${module.criterion.selected_criterion}}
  valid_criterion: ${module.criterion.available_criteria.${module.criterion.selected_criterion}}

  # For callbacks
  loss: ${module.criterion.train_criterion}

metrics:
  train: 
    _target_: "torchmetrics.Accuracy"
    task: "multiclass"
    num_classes: ${module.model.classifiers.config.num_classes}
  valid:
    _target_: "torchmetrics.Accuracy"
    task: "multiclass"
    num_classes: ${module.model.classifiers.config.num_classes}
  valid_best:
    _target_: torchmetrics.MaxMetric
  test:
    _target_: src.modules.metrics.metrics.VerificationMetrics

### optimizer and lr_scheduler ###
optimizer:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 1.0e-4
  weight_decay: 1.0e-5

scheduler_settings:
  available_lr_scheduler:
    warmup_cosine:
      _target_: torch.optim.lr_scheduler.SequentialLR
      schedulers:
        - _target_: torch.optim.lr_scheduler.LinearLR
          start_factor: 0.1
          total_iters: ${int:${mul:${trainer.max_epochs},0.1}}
        - _target_: torch.optim.lr_scheduler.CosineAnnealingLR
          T_max: ${int:${sub:${trainer.max_epochs},${module.scheduler_settings.available_lr_scheduler.warmup_cosine.schedulers[0].total_iters}}} # Remaining epochs after linear warmup
          eta_min: ${mul:${module.optimizer.lr},0.05}
      milestones:
        - ${int:${mul:${trainer.max_epochs},0.1}}  # 10% of total epochs are used for linear warmup, remaining for cosine annealing
      extras:
        monitor: ${replace:"valid/__metric__"}
        interval: epoch
        frequency: 1

    reduce_on_plateau:
      scheduler:
        _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
        mode: "max"
        factor: 0.3
        min_lr: 1.0e-7
        patience: 2
        verbose: True
      extras:
        monitor: ${replace:"valid/__metric__"}
        interval: "epoch"
        frequency: 1

  # Selected scheduler (defaults to reduce_on_plateau)
  selected_lr_scheduler: reduce_on_plateau

lr_scheduler: ${module.scheduler_settings.available_lr_scheduler.${module.scheduler_settings.selected_lr_scheduler}}

### extras and callbacks ###
logging_params:
  on_step: False
  on_epoch: True
  sync_dist: True
  prog_bar: True

batch_sizes:
  train: ${datamodule.loaders.train.batch_size}
  valid: ${datamodule.loaders.valid.batch_size}
  test: ${datamodule.loaders.test.batch_size}
