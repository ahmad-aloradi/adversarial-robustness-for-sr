_target_: src.modules.multimodal_vpc.MultiModalVPCModel

defaults:
  # - network: classification.yaml
  - _self_

model:
  audio_processor:
    _target_: torch.nn.Identity

  audio_encoder:
    _target_: speechbrain.inference.EncoderClassifier.from_hparams
    source: "speechbrain/spkrec-ecapa-voxceleb"
    savedir: "local/.pretrained_models/spkrec-ecapa-voxceleb"
    run_opts:
      device: "cuda"

  audio_processor_kwargs:
    return_tensors: "pt"
    sampling_rate: 16000

  text_processor:
    _target_: transformers.BertTokenizer.from_pretrained
    pretrained_model_name_or_path: "bert-base-uncased"

  text_encoder:
    _target_: transformers.BertModel.from_pretrained
    pretrained_model_name_or_path: ${module.model.text_processor.pretrained_model_name_or_path}
    num_labels: ${module.model.classifiers.classifiers_config.num_classes}

  text_processor_kwargs:
    return_tensors: "pt"
    padding: "max_length"
    truncation: True
    max_length: 512

  classifiers:
    fusion_classifier:
      _target_: src.modules.multimodal_vpc.FusionClassifierWithResiduals
      fuse_model:
        _target_: src.modules.multimodal_vpc.NormalizedWeightedSum
        audio_embedding_size: ${module.model.classifiers.classifiers_config.audio_embedding_size}
        text_embedding_size: ${module.model.classifiers.classifiers_config.text_embedding_size}
        bottleneck_size: ${module.model.classifiers.classifiers_config.bottleneck_size}
      input_size: ${module.model.classifiers.classifiers_config.fusion_classifier_input_size}
      hidden_size: ${module.model.classifiers.classifiers_config.fusion_classifier_hidden_size}
      num_classes: ${module.model.classifiers.classifiers_config.num_classes}
      dropout: 0.3
      norm_type: "batch"

    # fusion_classifier:
      # _target_: src.modules.multimodal_vpc.RobustFusionClassifier
      # audio_size: ${module.model.classifiers.classifiers_config.audio_embedding_size}
      # text_size: ${module.model.classifiers.classifiers_config.text_embedding_size}
      # hidden_size: ${module.model.classifiers.classifiers_config.bottleneck_size}
      # num_classes: ${module.model.classifiers.classifiers_config.num_classes}
      # dropout_audio: 0.3
      # dropout_text: 0.1

    classifiers_config:
      num_classes: ${datamodule.num_classes}
      bottleneck_size: 512 # Size of the bottleneck layer (embedding size)
      fusion_classifier_input_size: 512 # Depends on the fuser type
      fusion_classifier_hidden_size: 1024 # Increased hidden size for better representation
      audio_embedding_size: 192 # ECAPA embedding size
      text_embedding_size: 768  # BERT embedding size

  embedding_cache:
    max_size: 500000      # Maximum number of cached entries
    bypass_warmup: True  # Bypass warmup phase

criterion:
  train_criterion:
    _target_: src.modules.multimodal_vpc.EnhancedCriterion
    temperature: 0.1  # Increased temperature for more stable gradients
    # _target_: src.modules.multimodal_vpc.MultiModalFusionLoss
    # _target_: torch.nn.CrossEntropyLoss

  loss: ${module.criterion.train_criterion} # for callbacks

gradient_clip_val: 5.0

optimizer:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 1.0e-4
  weight_decay: 1.0e-5

# lr_scheduler:
#   scheduler:
#     _target_: torch.optim.lr_scheduler.OneCycleLR
#     max_lr: 1.0e-4
#     pct_start: 0.05  # 10% of training for warmup
#     anneal_strategy: "cos"
#     cycle_momentum: False
#   extras:
#     interval: "step"
#     frequency: 1

lr_scheduler:
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: "min"
    factor: 0.3
    min_lr: 1.0e-9
    patience: 2
    verbose: True
  extras:
    monitor: ${replace:"__metric__/valid"}/${module.metrics.valid_best.target_key}
    interval: "epoch"
    frequency: 1

metrics:
  train:
    _target_: "torchmetrics.Accuracy"
    task: "multiclass"
    num_classes: ${module.model.classifiers.classifiers_config.num_classes}
  valid:
    _target_: src.modules.metrics.metrics.VerificationMetrics
  test:
    _target_: src.modules.metrics.metrics.VerificationMetrics
  valid_best:
    _target_: src.modules.metrics.metrics.AutoSyncDictMinMetric
    target_key: "eer"


logging_params:
  on_step: False
  on_epoch: True
  sync_dist: True
  prog_bar: True

batch_sizes:
  train: ${datamodule.loaders.train.batch_size}
  valid: ${datamodule.loaders.valid.batch_size}
  test: ${datamodule.loaders.test.batch_size}
