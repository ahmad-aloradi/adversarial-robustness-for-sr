{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf1f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from speechbrain.utils.metric_stats import EER\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "sys.path.append('../.')\n",
    "from src.modules.metrics.metrics import VerificationMetrics, AS_norm\n",
    "from src.datamodules.components.vpc25.vpc_dataset import VPCTestCoallate, VPC25TestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3482d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp = Path('/home.local/aloradi/logs/train/runs/2025-02-02_22-12-58')\n",
    "anon_model = 'B3'\n",
    "data_df_file = 'test.csv'\n",
    "df_test = pd.read_csv(f'{str(model_exp / f\"vpc2025_artifacts/{data_df_file}\")}', sep=\"|\")\n",
    "\n",
    "\n",
    "# Load embeddings\n",
    "trials_embeds = 'test_embeds.pt'\n",
    "enrol_embeds = 'enrol_embeds.pt'\n",
    "cohort_embeds = 'valid_embeds.pt'\n",
    "\n",
    "test_path_tmp = list(model_exp.rglob(f'*/{trials_embeds}'))\n",
    "assert len(test_path_tmp) == 1, f'Expected one file called test_embeds.pt, found: {len(test_path_tmp)}'\n",
    "trials_path = test_path_tmp[0]\n",
    "\n",
    "enrol_tmp_tmp = list(model_exp.rglob(f'*/{enrol_embeds}'))\n",
    "assert len(enrol_tmp_tmp) == 1, f'Expected one file called test_embeds.pt, found: {len(enrol_tmp_tmp)}'\n",
    "enrol_path = enrol_tmp_tmp[0]\n",
    "\n",
    "cohort_path = list(model_exp.rglob(f'*/{cohort_embeds}'))[0]\n",
    "\n",
    "\n",
    "test_embeds = torch.load(str(trials_path))\n",
    "enrol_embeds = torch.load(str(enrol_path))\n",
    "\n",
    "cohort_embeddings = torch.load(str(cohort_path))\n",
    "cohort_embeddings = torch.stack(list(cohort_embeddings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70401439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_trials_path = 'test_trials.csv'\n",
    "test_trials_tmp = list(model_exp.rglob(f'*/{test_trials_path}'))\n",
    "assert len(test_trials_tmp) == 1, f'Expected one file called test_embeds.pt, found: {len(test_path_tmp)}'\n",
    "test_trials_csv = test_trials_tmp[0]\n",
    "\n",
    "test_data = VPC25TestDataset(data_dir='/home/aloradi/adversarial-robustness-for-sr/data/vpc2025_official',\n",
    "                             test_trials_path=test_trials_csv,\n",
    "                             sample_rate=16000,\n",
    "                             max_duration=None)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                         shuffle=False,\n",
    "                         batch_size=16,\n",
    "                         collate_fn=VPCTestCoallate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = VerificationMetrics()\n",
    "trials_results = []\n",
    "\n",
    "total_batches = len(test_loader)\n",
    "with tqdm(total=total_batches, desc=\"Test trials\") as pbar:\n",
    "    for batch in test_loader:\n",
    "        trial_embeddings = torch.stack([test_embeds[path] for path in batch.audio_path])\n",
    "        enroll_embeddings = torch.stack([enrol_embeds[model][enroll_id]\n",
    "                                    for model, enroll_id in zip(batch.model, batch.enroll_id)])\n",
    "        \n",
    "        raw_scores = torch.nn.functional.cosine_similarity(enroll_embeddings, trial_embeddings)\n",
    "\n",
    "        if cohort_embeddings is not None:\n",
    "            normalized_scores = []\n",
    "            for i, (enroll_emb, test_emb, model) in enumerate(zip(enroll_embeddings, trial_embeddings, batch.model)):\n",
    "                raw_score = raw_scores[i]\n",
    "                                \n",
    "                # Apply AS-Norm\n",
    "                norm_score = AS_norm(score=raw_score, \n",
    "                                     enroll_embedding=enroll_emb, \n",
    "                                     test_embedding=test_emb, \n",
    "                                     cohort_embeddings=cohort_embeddings, \n",
    "                                     topk=300)\n",
    "                normalized_scores.append(norm_score)\n",
    "            \n",
    "            # Convert back to tensor\n",
    "            normalized_scores = torch.tensor(normalized_scores, device=raw_scores.device)\n",
    "        \n",
    "        else:\n",
    "            normalized_scores = raw_scores.clone()        \n",
    "\n",
    "        # Update metric with normalized scores\n",
    "        metric.update(scores=normalized_scores, labels=torch.tensor(batch.trial_label))\n",
    "        \n",
    "        batch_dict = {\n",
    "            \"enrollment_id\": batch.enroll_id,\n",
    "            \"audio_path\": batch.audio_path,\n",
    "            \"label\": batch.trial_label,\n",
    "            \"score\": normalized_scores.tolist(),\n",
    "            \"model\": batch.model,\n",
    "        }\n",
    "        trials_results.append(batch_dict)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb006516",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([\n",
    "    {\n",
    "        \"enrollment_id\": enroll_id,\n",
    "        \"audio_path\": audio_path,\n",
    "        \"label\": label,\n",
    "        \"score\": score,\n",
    "        \"model\": model,                \n",
    "    }\n",
    "    for batch in trials_results\n",
    "    for enroll_id, audio_path, label, score, model in zip(\n",
    "        batch[\"enrollment_id\"],\n",
    "        batch[\"audio_path\"],\n",
    "        batch[\"label\"],\n",
    "        batch[\"score\"],\n",
    "        batch[\"model\"],\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deepcopy(scores)\n",
    "df['rel_filepath'] = df['audio_path'].str.extract(fr'({anon_model}.*)')\n",
    "df = df.merge(df_test[['speaker_id', 'rel_filepath', 'gender', 'recording_duration', 'text']], on='rel_filepath', how='left')\n",
    "\n",
    "# Compute EER for each enrollment speaker\n",
    "speakers = df['enrollment_id'].unique()\n",
    "genders = df['gender'].unique()\n",
    "eer_results_gender = {}\n",
    "\n",
    "metric_out_gender = {'male': [], 'female': []}\n",
    "gender_figures =  {'male': [], 'female': []}\n",
    "\n",
    "    \n",
    "######## Gender-level EER (Official evaluation) #########\n",
    "for gender in genders:\n",
    "    metric.reset()\n",
    "    \n",
    "    speaker_data = df[df['gender'] == gender]\n",
    "    scores = speaker_data['score'].values\n",
    "    labels = speaker_data['label'].values\n",
    "    \n",
    "    metric.update(torch.from_numpy(scores), torch.from_numpy(labels))\n",
    "    metrics = metric.compute()\n",
    "    gender_figures[gender] = metric.plot_curves()\n",
    "    \n",
    "    metric_out_gender[gender] = metrics\n",
    "    eer_results_gender[gender] = metrics['eer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comfort)",
   "language": "python",
   "name": "comfort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
