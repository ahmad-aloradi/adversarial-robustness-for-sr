{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825e9a5-a1dd-45c4-a90b-3fc597b26997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from speechbrain.augment.time_domain import DropChunk, DropFreq, AddReverb, AddNoise, DropBitResolution, SpeedPerturb\n",
    "from speechbrain.processing.features import InputNormalization\n",
    "from speechbrain.lobes.features import Fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72141b96-1c08-4441-9f4a-949f05802cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ahmad/adversarial-robustness-for-sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d426202-58a5-40e9-9614-de9850422e4f",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5296a-42c8-442b-8889-b13c3ad99dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'data/voxceleb/voxceleb1_2/id00012/21Uxsk56VDQ/00001.wav'\n",
    "path2 = 'data/voxceleb/voxceleb1_2/id00012/21Uxsk56VDQ/00002.wav'\n",
    "\n",
    "audio1, sr1 = sf.read(path1)\n",
    "audio2, sr2 = sf.read(path2)\n",
    "\n",
    "assert sr1 == sr2\n",
    "sr = sr1\n",
    "\n",
    "min_len = min(len(audio1), len(audio2))\n",
    "if min_len == len(audio1):\n",
    "    audio2 = audio2[: min_len]\n",
    "else:\n",
    "    audio1 = audio1[: min_len]\n",
    "\n",
    "audio1 = torch.tensor(audio1).float().unsqueeze(0)\n",
    "audio2 = torch.tensor(audio2).float().unsqueeze(0)\n",
    "audio = torch.cat((audio1, audio2), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0b4a7-b341-4e5b-ae81-74fd4900fdd2",
   "metadata": {},
   "source": [
    "## Utils Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0601b4-b3f0-4be0-aa04-fb365f6e9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(waveform: torch.Tensor, method: str = \"rms\", target_level: float = -20.0,\n",
    "                    clip_limit: float = 0.999, epsilon: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize audio using various methods to handle different scenarios better than simple max normalization.\n",
    "    \n",
    "    Args:\n",
    "        waveform: Input waveform tensor of shape (1, samples)\n",
    "        method: Normalization method: 'peak', 'rms', 'percentile', or 'dynamic'\n",
    "        target_level: Target level in dB for RMS normalization (typically -20 dB)\n",
    "        clip_limit: Clipping limit for preventing excessive amplification (0-1)\n",
    "        epsilon: Small value to prevent division by zero\n",
    "        \n",
    "    Returns:\n",
    "        Normalized waveform tensor of shape (1, samples)\n",
    "    \"\"\"\n",
    "    waveform = waveform.squeeze(0)\n",
    "    assert waveform.dim() == 1, \"Expected single-channel audio\"\n",
    "    \n",
    "    # Center the waveform by removing DC offset\n",
    "    waveform = waveform - waveform.mean()\n",
    "    \n",
    "    if method == \"peak\":\n",
    "        # Traditional peak normalization\n",
    "        peak = waveform.abs().max() + epsilon\n",
    "        waveform = waveform / peak\n",
    "        \n",
    "    elif method == \"rms\":\n",
    "        # RMS normalization (based on signal energy)\n",
    "        rms = torch.sqrt(torch.mean(waveform ** 2))\n",
    "        target_rms = 10 ** (target_level / 20)  # Convert dB to linear\n",
    "        gain = target_rms / (rms + epsilon)\n",
    "        waveform = waveform * gain\n",
    "        \n",
    "    elif method == \"percentile\":\n",
    "        # Percentile-based normalization (robust to outliers)\n",
    "        sorted_abs = torch.sort(waveform.abs())[0]\n",
    "        idx = min(int(len(sorted_abs) * 0.995), len(sorted_abs) - 1)\n",
    "        ref_level = sorted_abs[idx] + epsilon\n",
    "        waveform = waveform / ref_level\n",
    "        \n",
    "    elif method == \"dynamic\":\n",
    "        # Dynamic range compression (logarithmic compression)\n",
    "        sign = torch.sign(waveform)\n",
    "        abs_wave = waveform.abs() + epsilon\n",
    "        compressed = sign * torch.log1p(abs_wave) / torch.log1p(torch.tensor(1.0))\n",
    "        peak = compressed.abs().max() + epsilon\n",
    "        waveform = compressed / peak\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization method: {method}\")\n",
    "    \n",
    "    # Apply clipping to prevent excessive values\n",
    "    if clip_limit < 1.0:\n",
    "        waveform = torch.clamp(waveform, min=-clip_limit, max=clip_limit)\n",
    "    \n",
    "    return waveform.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Function that returns the HTML for an audio player\n",
    "def audio_player_html(audio_array, sr):\n",
    "    # Create an IPython Audio object and return its HTML representation\n",
    "    return Audio(audio_array, rate=sr)._repr_html_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f72ce-5763-4946-9402-ef8c986c0dc1",
   "metadata": {},
   "source": [
    "## Waveform Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016e02a-1323-4ee6-b1de-c0e5ca7d26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized waveforms\n",
    "norm_method = 'rms'\n",
    "audio1_norm = normalize_audio(audio1, norm_method)\n",
    "audio2_norm = normalize_audio(audio2, norm_method)\n",
    "audio_norm = torch.cat((audio1_norm, audio2_norm), dim=0)\n",
    "\n",
    "audio_data = {\n",
    "    'Sample': [f\"{path1[path1.index('voxceleb1_2') + len('voxceleb1_2') + 1:]}\", \n",
    "               f\"{path2[path2.index('voxceleb1_2') + len('voxceleb1_2') + 1:]}\"\n",
    "              ],\n",
    "    'Raw Audio': [audio[0], audio[1]],\n",
    "    f'{norm_method.upper()} Normalized Audio': [audio_norm[0], audio_norm[1]],\n",
    "}\n",
    "\n",
    "# Build the HTML table dynamically\n",
    "headers = list(audio_data.keys())\n",
    "table_html = '<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">'\n",
    "\n",
    "# Create header row with centered headings\n",
    "table_html += '<tr>'\n",
    "for header in headers:\n",
    "    table_html += f'<th style=\"padding: 8px; text-align: center;\">{header}</th>'\n",
    "table_html += '</tr>'\n",
    "\n",
    "# Determine the number of samples (assuming all lists are the same length)\n",
    "n_samples = len(audio_data['Sample'])\n",
    "\n",
    "# Create table rows for each sample\n",
    "for i in range(n_samples):\n",
    "    row_html = '<tr>'\n",
    "    for key in headers:\n",
    "        if key == 'Sample':\n",
    "            row_html += f'<td style=\"padding: 8px;\">{audio_data[key][i]}</td>'\n",
    "        else:\n",
    "            audio_array = audio_data[key][i]\n",
    "            player_html = audio_player_html(audio_array, sr)\n",
    "            row_html += f'<td style=\"padding: 8px;\">{player_html}</td>'\n",
    "    row_html += '</tr>'\n",
    "    table_html += row_html\n",
    "\n",
    "table_html += '</table>'\n",
    "\n",
    "# Display the complete table in the Jupyter Notebook\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92fa81e-629a-48f7-ac2e-3278f7d50ab7",
   "metadata": {},
   "source": [
    "## Features Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42f7c9-9779-4b37-b1de-91b254ea498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features \n",
    "audio_processor = Fbank(sample_rate=16000, deltas=False, n_mels=80, f_min=0, f_max=None,\n",
    "                        n_fft=512, win_length=25, hop_length=10, left_frames=0, right_frames=0)\n",
    "audio_feats = audio_processor(audio)\n",
    "audio_norm_feats = audio_processor(audio_norm)\n",
    "\n",
    "# Features normalization\n",
    "mfcc_norm = InputNormalization(norm_type='sentence', std_norm=False)\n",
    "audio_feats_norm = mfcc_norm(audio_feats, lengths=torch.tensor([1, 1]))\n",
    "audio_norm_feats_norm = mfcc_norm(audio_norm_feats, lengths=torch.tensor([1, 1]))\n",
    "\n",
    "# Plot features\n",
    "T_max = 320\n",
    "\n",
    "plt.imshow(audio_feats[1, :T_max, ...].T, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(audio_norm_feats[1, :T_max, ...].T, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(audio_feats_norm[1, :T_max, ...].T, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(audio_norm_feats_norm[1, :T_max, ...].T, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(abs(audio_feats_norm[1, :T_max, ...] - audio_norm_feats_norm[1, :T_max, ...]).T, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(abs(audio_norm_feats[1, :T_max, ...] - audio_feats[1, :T_max, ...]).T, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914dc26-5942-4008-b7a2-961355c33e53",
   "metadata": {},
   "source": [
    "## DropChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d17d3-1d72-4815-bae9-8aa925e7bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_dropper = DropChunk(drop_length_low=2000, drop_length_high=8000)\n",
    "chunk_dropped = chunk_dropper(audio, torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2e79a-a72a-4ee2-834d-44c4178e1cd0",
   "metadata": {},
   "source": [
    "## DropFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98543261-25d8-4bc0-b29f-9cf89f36bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dropper = DropFreq(drop_freq_count_low=1, drop_freq_count_high=3)\n",
    "freq_dropped = freq_dropper(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33bf93-d8ee-40ec-baef-49f7ebcf8b3a",
   "metadata": {},
   "source": [
    "## AddReverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9153a7-15eb-49a2-ab53-ca6d2d8a4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb = AddReverb('data/voxceleb/RIRS_NOISES/reverb.csv')\n",
    "reverbed = reverb(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600c8b9-14fb-4719-ba93-f8e6bf737ec9",
   "metadata": {},
   "source": [
    "## AddNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099b648-563f-4752-afa9-e77a7e8a98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisifier = AddNoise('data/voxceleb/RIRS_NOISES/noise.csv')\n",
    "noisy = noisifier(audio, lengths=torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf313157-36f6-45c5-83ad-8352001746fc",
   "metadata": {},
   "source": [
    "## DropBitResolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3b8c6-8d00-41f6-863f-02effc163989",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_dropper = DropBitResolution()\n",
    "bit_dropped = bit_dropper(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc4fb0-1a60-49ab-8ddc-3d0988bb35fc",
   "metadata": {},
   "source": [
    "## SpeedPerturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76dc517-8fff-4fe0-9ea7-783467e7d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_perturber = SpeedPerturb(orig_freq=sr, speeds=[90, 110], device='cuda')\n",
    "speeded = speed_perturber(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5827e-5957-4253-8c08-01f5eec0c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for different samples under different conditions\n",
    "audio_data = {\n",
    "    'Sample': [f\"{path1[path1.index('voxceleb1_2') + len('voxceleb1_2') + 1:]}\", \n",
    "               f\"{path2[path2.index('voxceleb1_2') + len('voxceleb1_2') + 1:]}\"\n",
    "              ],\n",
    "    'Clean': [audio[0], audio[1] ],\n",
    "    'Noisy': [noisy[0], noisy[1]],\n",
    "    'Reverberated': [reverbed[0], reverbed[1]],\n",
    "    'Frequency Dropped': [freq_dropped[0], freq_dropped[1] ],\n",
    "    'Chunks Dropped': [chunk_dropped[0], chunk_dropped[1]],\n",
    "    'Bit Dropped': [bit_dropped[0], bit_dropped[1]],\n",
    "    'Speed Pertubed': [speeded[0], speeded[1]]\n",
    "}\n",
    "\n",
    "# Build the HTML table dynamically\n",
    "headers = list(audio_data.keys())\n",
    "table_html = '<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">'\n",
    "\n",
    "# Create header row with centered headings\n",
    "table_html += '<tr>'\n",
    "for header in headers:\n",
    "    table_html += f'<th style=\"padding: 8px; text-align: center;\">{header}</th>'\n",
    "table_html += '</tr>'\n",
    "\n",
    "# Determine the number of samples (assuming all lists are the same length)\n",
    "n_samples = len(audio_data['Sample'])\n",
    "\n",
    "# Create table rows for each sample\n",
    "for i in range(n_samples):\n",
    "    row_html = '<tr>'\n",
    "    for key in headers:\n",
    "        if key == 'Sample':\n",
    "            row_html += f'<td style=\"padding: 8px;\">{audio_data[key][i]}</td>'\n",
    "        else:\n",
    "            audio_array = audio_data[key][i]\n",
    "            player_html = audio_player_html(audio_array, sr)\n",
    "            row_html += f'<td style=\"padding: 8px;\">{player_html}</td>'\n",
    "    row_html += '</tr>'\n",
    "    table_html += row_html\n",
    "\n",
    "table_html += '</table>'\n",
    "\n",
    "# Display the complete table in the Jupyter Notebook\n",
    "display(HTML(table_html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
