{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from speechbrain.utils.metric_stats import EER\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "sys.path.append('../.')\n",
    "from src.modules.metrics.metrics import VerificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = '2025-02-15_22-03-15' # 2025-02-02_22-12-58, 2025-02-09_01-18-15 2025-02-12_14-13-10 2025-02-15_22-03-15\n",
    "MODELS_PATH = '/home.local/aloradi/logs/train/runs'\n",
    "model_exp = Path(f'{MODELS_PATH}/{model_filename}')\n",
    "eval_test = True\n",
    "\n",
    "scores_csv_file = 'test_scores.csv' if eval_test else'valid_scores.csv'\n",
    "data_df_file = 'test.csv' if eval_test else'dev.csv'\n",
    "\n",
    "tmp = list(model_exp.rglob(f'*/{scores_csv_file}'))\n",
    "assert len(tmp) == 1, f'Expected one file called test_scores.csv, found: {len(tmp)}'\n",
    "scores_path = tmp[0]\n",
    "\n",
    "df = pd.read_csv(f'{scores_path}')\n",
    "df_test = pd.read_csv(f'{str(model_exp / f\"vpc2025_artifacts/{data_df_file}\")}', sep=\"|\")\n",
    "\n",
    "df['rel_filepath'] = df['audio_path'].apply(lambda x: x.split('vpc2025_official/')[-1])\n",
    "df = df.merge(df_test[['speaker_id', 'rel_filepath', 'gender', 'recording_duration', 'text']], on='rel_filepath', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radar_plot(eer_results_speaker,\n",
    "                      output_path,\n",
    "                      title=\"Speaker-specific EER\",\n",
    "                      description=\"Equal Error Rate (EER) across different speakers\"):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.6, 0.8], polar=True)\n",
    "    \n",
    "    labels = list(eer_results_speaker.keys())\n",
    "    values = list(eer_results_speaker.values())\n",
    "    labels = list(eer_results_speaker.keys())\n",
    "    values = list(eer_results_speaker.values())\n",
    "    num_vars = len(labels)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    \n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax.plot(angles, values, color='#FF6B6B', linewidth=2, marker='o', \n",
    "           markersize=8, label='EER Values')\n",
    "    \n",
    "    ax.grid(color='gray', alpha=0.2, linestyle='--', linewidth=1)\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_vars):\n",
    "        angle_rad = angles[i]\n",
    "        angle_deg = angle_rad * 180 / np.pi\n",
    "        \n",
    "        if angle_deg > 90 and angle_deg < 270:\n",
    "            rotation = angle_deg + 180\n",
    "        else:\n",
    "            rotation = angle_deg\n",
    "            \n",
    "        ax.text(angle_rad, values[i] + max(values) * 0.1,\n",
    "                f'{values[i]:.2f}',\n",
    "                ha='center', va='center',\n",
    "                rotation=rotation,\n",
    "                fontsize=9,\n",
    "                bbox=dict(facecolor='white', \n",
    "                         edgecolor='none',\n",
    "                         alpha=0.8,\n",
    "                         pad=2))\n",
    "    \n",
    "    plt.title(title, pad=20, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "    plt.savefig(output_path, \n",
    "                dpi=300, \n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5,\n",
    "                format='png',\n",
    "                transparent=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gender/speaker metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gender_metrics(df: pd.DataFrame) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Compute verification metrics for each gender.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['gender', score', 'label']\n",
    "    \n",
    "    Returns:\n",
    "        gender_metrics: Dict of metrics per gender\n",
    "        gender_curves: Dict of curve data per gender\n",
    "        gender_eer: Dict of EER values per gender\n",
    "    \"\"\"\n",
    "    gender_metrics = {}\n",
    "    gender_curves = {}\n",
    "    gender_eer = {}\n",
    "    metric = VerificationMetrics()\n",
    "    \n",
    "    for gender in df['gender'].unique():\n",
    "        metric.reset()\n",
    "        gender_data = df[df['gender'] == gender]\n",
    "        \n",
    "        # Compute metrics\n",
    "        metric.update(\n",
    "            torch.from_numpy(gender_data['score'].values),\n",
    "            torch.from_numpy(gender_data['label'].values)\n",
    "        )\n",
    "        metrics = metric.compute()\n",
    "        \n",
    "        # Store results\n",
    "        gender_metrics[gender] = metrics\n",
    "        gender_curves[gender] = {\n",
    "            k: v.detach().cpu().numpy() \n",
    "            for k, v in metric._curve_data.items()\n",
    "        }\n",
    "        gender_eer[gender] = metrics['eer']\n",
    "        \n",
    "    return gender_metrics, gender_curves, gender_eer\n",
    "\n",
    "\n",
    "def plot_gender_det_curves(gender_curves: Dict, gender_metrics: Dict, eps: float = 1e-8) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot DET curves for multiple speakers.\n",
    "    \n",
    "    Args:\n",
    "        gender_curves: Dict of curve data per speaker\n",
    "        gender_metrics: Dict of metrics per speaker\n",
    "        eps: Small value for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Get sorted genders for consistent ordering\n",
    "    genders = sorted(gender_curves.keys())\n",
    "    \n",
    "    # Define fixed colors for consistency\n",
    "    GENDER_COLORS = {'female': '#FF69B4','male': '#4169E1'}\n",
    "    \n",
    "    # Plot each speaker's curve\n",
    "    mean_eer = []\n",
    "    for gender in genders:\n",
    "        curves = gender_curves[gender]\n",
    "        metrics = gender_metrics[gender]\n",
    "        \n",
    "        far = np.maximum(curves['far'], eps)\n",
    "        frr = np.maximum(curves['frr'], eps)\n",
    "        eer = metrics['eer']\n",
    "        mean_eer.append(eer)\n",
    "        \n",
    "        ax.plot(far, frr, '-', color=GENDER_COLORS.get(gender, 'gray'), alpha=0.5, linewidth=1, label=f'{gender} (EER: {eer:.3f})')\n",
    "    \n",
    "    # Plot mean EER point for visual clarity\n",
    "    mean_eer_value = np.mean(mean_eer)\n",
    "    ax.plot(mean_eer_value, mean_eer_value, 'ko', markersize=8, label=f'Mean EER: {mean_eer_value:.3f}')\n",
    "    \n",
    "    # Plot diagonal\n",
    "    ax.plot([eps, 1], [eps, 1], 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('False Acceptance Rate (FAR)')\n",
    "    ax.set_ylabel('False Rejection Rate (FRR)')\n",
    "    ax.set_title('Detection Error Tradeoff (DET) Curves by Gender')\n",
    "    ax.grid(True, which='both', linestyle='--', alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=True, fancybox=False, edgecolor='black')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_speaker_metrics(df: pd.DataFrame) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Compute verification metrics for each speaker.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['enrollment_id', 'speaker_id', 'score', 'label']\n",
    "    \n",
    "    Returns:\n",
    "        speaker_metrics: Dict of metrics per speaker\n",
    "        speaker_curves: Dict of curve data per speaker\n",
    "        speaker_eer: Dict of EER values per speaker ID\n",
    "    \"\"\"\n",
    "    speaker_metrics = {}\n",
    "    speaker_curves = {}\n",
    "    speaker_eer = {}\n",
    "    metric = VerificationMetrics()\n",
    "    \n",
    "    for speaker in df['enrollment_id'].unique():\n",
    "        metric.reset()\n",
    "        speaker_data = df[df['speaker_id'] == speaker]\n",
    "        \n",
    "        # Compute metrics\n",
    "        metric.update(\n",
    "            torch.from_numpy(speaker_data['score'].values),\n",
    "            torch.from_numpy(speaker_data['label'].values)\n",
    "        )\n",
    "        metrics = metric.compute()\n",
    "        \n",
    "        # Store results\n",
    "        speaker_metrics[speaker] = metrics\n",
    "        speaker_curves[speaker] = {\n",
    "            k: v.detach().cpu().numpy() \n",
    "            for k, v in metric._curve_data.items()\n",
    "        }\n",
    "        speaker_eer[speaker.split('_')[-1]] = metrics['eer']\n",
    "        \n",
    "    return speaker_metrics, speaker_curves, speaker_eer\n",
    "\n",
    "\n",
    "def plot_speaker_det_curves(speaker_curves: Dict, \n",
    "                            speaker_metrics: Dict,\n",
    "                            eps: float = 1e-8,\n",
    "                            max_speakers: int = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot DET curves for multiple speakers.\n",
    "    \n",
    "    Args:\n",
    "        speaker_curves: Dict of curve data per speaker\n",
    "        speaker_metrics: Dict of metrics per speaker\n",
    "        eps: Small value for numerical stability\n",
    "        max_speakers: Maximum number of speakers to plot (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Select speakers to plot\n",
    "    speakers = sorted(speaker_curves.keys())\n",
    "    if max_speakers:\n",
    "        speakers = speakers[:max_speakers]\n",
    "    \n",
    "    # Get color map for speakers\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(speakers)))\n",
    "    \n",
    "    # Plot each speaker's curve\n",
    "    mean_eer = []\n",
    "    for speaker, color in zip(speakers, colors):\n",
    "        curves = speaker_curves[speaker]\n",
    "        metrics = speaker_metrics[speaker]\n",
    "        \n",
    "        far = np.maximum(curves['far'], eps)\n",
    "        frr = np.maximum(curves['frr'], eps)\n",
    "        eer = metrics['eer']\n",
    "        mean_eer.append(eer)\n",
    "        \n",
    "        ax.plot(far, frr, '-', color=color, alpha=0.5, linewidth=1,\n",
    "               label=f'Speaker {speaker.split(\"_\")[-1]} (EER: {eer:.3f})')\n",
    "    \n",
    "    # Plot mean EER point for visual clarity\n",
    "    mean_eer_value = np.mean(mean_eer)\n",
    "    ax.plot(mean_eer_value, mean_eer_value, 'ko', markersize=8,\n",
    "           label=f'Mean EER: {mean_eer_value:.3f}')\n",
    "    \n",
    "    # Plot diagonal\n",
    "    ax.plot([eps, 1], [eps, 1], 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('False Acceptance Rate (FAR)')\n",
    "    ax.set_ylabel('False Rejection Rate (FRR)')\n",
    "    ax.set_title('Detection Error Tradeoff (DET) Curves by Speaker')\n",
    "    ax.grid(True, which='both', linestyle='--', alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "             frameon=True, fancybox=False, edgecolor='black')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_speaker_results(df: pd.DataFrame, output_path: str, max_speakers: int = None):\n",
    "    \"\"\"\n",
    "    Analyze and plot verification results from scores file.\n",
    "    \n",
    "    Args:\n",
    "        df: CSV dataframe\n",
    "        output_path: path to save the figure\n",
    "        max_speakers: Maximum number of speakers to plot\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    speaker_metrics, speaker_curves, speaker_eer = compute_speaker_metrics(df)\n",
    "    \n",
    "    # Plot DET curves\n",
    "    det_fig = plot_speaker_det_curves(\n",
    "        speaker_curves, \n",
    "        speaker_metrics,\n",
    "        max_speakers=max_speakers\n",
    "    )\n",
    "    plt.savefig(output_path,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5,\n",
    "                format='png',\n",
    "                transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return det_fig, speaker_metrics, speaker_eer\n",
    "\n",
    "\n",
    "def analyze_gender_results(df: pd.DataFrame, output_path: str):\n",
    "    \"\"\"\n",
    "    Analyze and plot verification results from scores file.\n",
    "    \n",
    "    Args:\n",
    "        df: CSV dataframe\n",
    "        output_path: path to save the figure\n",
    "        max_speakers: Maximum number of speakers to plot\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    gender_metrics, gender_curves, gender_eer = compute_gender_metrics(df)\n",
    "    \n",
    "    # Plot DET curves\n",
    "    det_fig = plot_gender_det_curves(gender_curves, gender_metrics)\n",
    "    \n",
    "    plt.savefig(output_path,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5,\n",
    "                format='png',\n",
    "                transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return det_fig, gender_metrics, gender_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Trial Scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_count(data):\n",
    "    \"\"\"Calculate optimal number of bins (Freedman-Diaconis rule)\"\"\"\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    bin_width = 2 * iqr * len(data)**(-1/3)  # Freedman-Diaconis rule\n",
    "    return int(np.ceil((data.max() - data.min()) / bin_width)) if bin_width > 0 else 30\n",
    "\n",
    "\n",
    "def plot_score_distribution(df_test, score_schemes=None, figsize=(10, 6), save_dir=None, show_plot=True):\n",
    "    \"\"\"\n",
    "    Plot the distribution of similarity scores for genuine and impostor trials.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_test : pandas.DataFrame\n",
    "        DataFrame containing similarity scores and labels\n",
    "    score_schemes : list, optional\n",
    "        List of score column names to plot (default: ['score', 'norm_score'] or available columns)\n",
    "    figsize : tuple, optional\n",
    "        Figure size as (width, height) in inches (default: (10, 6))\n",
    "    save_dir : str, optional\n",
    "        Directory path to save plots. If None, plots won't be saved.\n",
    "    show_plot : bool, optional\n",
    "        Whether to display the plot interactively (default: True)\n",
    "    \"\"\"\n",
    "    # Set publication-quality font settings\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Times New Roman', 'DejaVu Serif', 'Palatino', 'Computer Modern Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 12,\n",
    "        'figure.titlesize': 18,\n",
    "        'text.usetex': False,  # Set to True if you have LaTeX installed\n",
    "        'axes.linewidth': 1.2,\n",
    "        'xtick.major.width': 1.2,\n",
    "        'ytick.major.width': 1.2,\n",
    "        'xtick.major.size': 5,\n",
    "        'ytick.major.size': 5\n",
    "    })\n",
    "    \n",
    "    # Set default score schemes if not provided\n",
    "    if score_schemes is None:\n",
    "        default_schemes = ['score', 'norm_score']\n",
    "        # Filter to only keep schemes that exist in the dataframe\n",
    "        score_schemes = [scheme for scheme in default_schemes if scheme in df_test.columns]\n",
    "        \n",
    "        # If none of the default schemes exist, try to find any columns that might be scores\n",
    "        if not score_schemes:\n",
    "            # Look for columns that might contain scores (excluding 'label')\n",
    "            potential_scores = [col for col in df_test.columns if col != 'label']\n",
    "            if potential_scores:\n",
    "                score_schemes = potential_scores[:2]  # Take at most 2 columns\n",
    "                print(f\"Using detected score columns: {score_schemes}\")\n",
    "            else:\n",
    "                raise ValueError(\"No score columns found in the dataframe.\")\n",
    "    else:\n",
    "        # Filter user-provided schemes to only keep those that exist in the dataframe\n",
    "        valid_schemes = [scheme for scheme in score_schemes if scheme in df_test.columns]\n",
    "        if not valid_schemes:\n",
    "            raise ValueError(f\"None of the provided score schemes {score_schemes} exist in the dataframe.\")\n",
    "        \n",
    "        if len(valid_schemes) < len(score_schemes):\n",
    "            print(f\"Warning: Only using valid columns: {valid_schemes}. Skipping non-existent columns.\")\n",
    "        \n",
    "        score_schemes = valid_schemes\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for score_scheme in score_schemes:\n",
    "        # Set a professional style with whitegrid for clarity\n",
    "        sns.set_style('whitegrid', {\n",
    "            'grid.linestyle': '--',\n",
    "            'grid.alpha': 0.7,\n",
    "            'axes.edgecolor': '0.2',\n",
    "            'axes.grid': True\n",
    "        })\n",
    "        \n",
    "        # Create figure with higher resolution for publication\n",
    "        fig = plt.figure(figsize=figsize, dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Extract scores by label\n",
    "        neg_scores = df_test[df_test.label == 0][score_scheme]\n",
    "        pos_scores = df_test[df_test.label == 1][score_scheme]\n",
    "        \n",
    "        # Get count of trials\n",
    "        neg_count = len(neg_scores)\n",
    "        pos_count = len(pos_scores)\n",
    "        total_count = neg_count + pos_count\n",
    "        \n",
    "        # Calculate appropriate bin count with upper limit\n",
    "        bins = max(get_bin_count(neg_scores), get_bin_count(pos_scores))\n",
    "#         bins = min(bins, 100)  # Cap at 100 bins for visualization clarity\n",
    "        \n",
    "        # Calculate statistics once\n",
    "        neg_mean = neg_scores.mean()\n",
    "        pos_mean = pos_scores.mean()\n",
    "        separation = pos_mean - neg_mean\n",
    "        \n",
    "        # Create normalized histograms with more professional colors\n",
    "        ax.hist(neg_scores, bins=bins, alpha=0.7, density=True, \n",
    "                color='#D45E5E', edgecolor='#8B0000', linewidth=1.2, \n",
    "                label=f'Impostor Trials (n={neg_count:,})')\n",
    "        ax.hist(pos_scores, bins=bins, alpha=0.7, density=True,\n",
    "                color='#5E81D4', edgecolor='#00008B', linewidth=1.2,\n",
    "                label=f'Genuine Trials (n={pos_count:,})')\n",
    "        \n",
    "        # Add vertical lines for means with publication-quality styling\n",
    "        ax.axvline(neg_mean, color='#8B0000', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Impostor Mean: {neg_mean:.3f}')\n",
    "        ax.axvline(pos_mean, color='#00008B', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Genuine Mean: {pos_mean:.3f}')\n",
    "        \n",
    "        # Add labels and title with improved formatting\n",
    "        score_name = score_scheme.replace('_', ' ').title()\n",
    "        ax.set_xlabel('Similarity Score', fontweight='bold')\n",
    "        ax.set_ylabel('Normalized Frequency', fontweight='bold')\n",
    "        ax.set_title(f'Distribution of Similarity Scores by Trial Type ({score_name})', \n",
    "                    fontweight='bold', pad=15)\n",
    "        \n",
    "        # Customize legend with cleaner appearance\n",
    "        legend = ax.legend(frameon=True, fancybox=False, framealpha=0.95, \n",
    "                          edgecolor='0.2', loc='best')\n",
    "        legend.get_frame().set_linewidth(1.0)\n",
    "        \n",
    "        # Add annotation about the separation and trial counts with cleaner styling\n",
    "        info_text = f\"Separation: {separation:.3f}\\nTotal Trials: {total_count:,}\\nImpostor: {neg_count:,} ({neg_count/total_count:.1%})\\nGenuine: {pos_count:,} ({pos_count/total_count:.1%})\"\n",
    "\n",
    "        ax.annotate(info_text,\n",
    "                   xy=(0.97, 0.97), xycoords='axes fraction', \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"#F8F8F8\", ec=\"gray\", \n",
    "                            alpha=0.95, linewidth=1.2),\n",
    "                   ha='right', va='top', fontweight='normal',  # Changed from 'bold' to 'normal'\n",
    "                   fontsize=plt.rcParams['legend.fontsize'])   # Match legend font size\n",
    "        \n",
    "        # Customize spines and ticks for publication quality\n",
    "        for spine in ['top', 'right', 'bottom', 'left']:\n",
    "            ax.spines[spine].set_linewidth(1.2)\n",
    "        ax.tick_params(width=1.2, length=5, direction='out')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot if directory is provided\n",
    "        if save_dir:\n",
    "            # Create filename from score scheme with high resolution\n",
    "            filename = f\"{score_scheme.replace(' ', '_')}_distribution.pdf\"  # PDF for vector quality\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            plt.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "            \n",
    "            # Also save a PNG version for easy viewing\n",
    "            png_filepath = os.path.join(save_dir, f\"{score_scheme.replace(' ', '_')}_distribution.png\")\n",
    "            plt.savefig(png_filepath, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            print(f\"Saved plot to {filepath} and {png_filepath}\")\n",
    "        \n",
    "        # Show the plot if requested\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis_header(anon_model: str):\n",
    "    header = f\"\"\"\n",
    "    <div style=\"background-color: #f0f2f6; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h3 style=\"color: #2c3e50; margin: 0; text-align: center;\">\n",
    "            Analyzing Anonymization System: {anon_model}\n",
    "        </h3>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(header))\n",
    "\n",
    "def print_analysis_results(speaker_eer, gender_eer):\n",
    "    results = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h4 style=\"color: #2c3e50; margin: 0 0 10px 0;\">Results Summary:</h4>\n",
    "        <p style=\"margin: 5px 0; color: #34495e;\">\n",
    "            <b>Speaker Results:</b><br>\n",
    "            Mean EER: {np.mean(list(speaker_eer.values())):.3f} ± {np.std(list(speaker_eer.values())):.3f}\n",
    "        </p>\n",
    "        <p style=\"margin: 5px 0; color: #34495e;\">\n",
    "            <b>Gender Results:</b><br>\n",
    "            Mean EER: {np.mean(list(gender_eer.values())):.3f} ± {np.std(list(gender_eer.values())):.3f}\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tot_speaker_metrics = {}\n",
    "tot_gender_metrics = {}\n",
    "results_dir = f'results/{model_filename}'   # model_exp (maybe)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "for anon_model in df['model'].unique():\n",
    "    df_test = df[df['model'] == anon_model]\n",
    "    \n",
    "    print_analysis_header(anon_model)\n",
    "\n",
    "    # Plot scores histogram\n",
    "    plot_score_distribution(df_test, figsize=(12, 8), save_dir=f'{results_dir}/{anon_model}', show_plot=True)\n",
    "    \n",
    "    fig_speaker, speaker_metrics, speaker_eer = analyze_speaker_results(\n",
    "        df_test, output_path=f'{results_dir}/{anon_model}/speaker_DET_{anon_model}.png', max_speakers=None)\n",
    "    fig_gender, gender_metrics, gender_eer = analyze_gender_results(\n",
    "        df_test, output_path=f'{results_dir}/{anon_model}/gender_DET_{anon_model}.png')\n",
    "    \n",
    "    print_analysis_results(speaker_eer, gender_eer)\n",
    "    \n",
    "    # Save results as csvs\n",
    "    tot_speaker_metrics[anon_model] = pd.DataFrame.from_records(speaker_metrics).astype(float)\n",
    "    tot_gender_metrics[anon_model] = pd.DataFrame.from_records(gender_metrics).astype(float)\n",
    "    tot_speaker_metrics[anon_model].to_csv(f'{results_dir}/{anon_model}/per_speaker_results_{anon_model}.csv')\n",
    "    tot_gender_metrics[anon_model].to_csv(f'{results_dir}/{anon_model}/per_gender_results_{anon_model}.csv')\n",
    "\n",
    "    # Create radar plot\n",
    "    create_radar_plot(speaker_eer, output_path=f'{results_dir}/{anon_model}/speaker_eer_{anon_model}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comfort)",
   "language": "python",
   "name": "comfort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
